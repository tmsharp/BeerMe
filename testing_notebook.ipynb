{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "db_path = 'data/beer.db'\n",
    "\n",
    "def username_options(database_path=db_path):\n",
    "\n",
    "    query = \"SELECT DISTINCT username FROM prepped_data ORDER BY username\"\n",
    "    \n",
    "    with sqlite3.connect(database_path) as conn:\n",
    "        usernames = list(pd.read_sql(query, conn)['username'])\n",
    "\n",
    "    username_options = [{'label': username, 'value': username} for username in usernames]\n",
    "\n",
    "    return username_options\n",
    "\n",
    "\n",
    "def beer_options(database_path=db_path):\n",
    "    query = \"SELECT DISTINCT beer_name FROM prepped_data ORDER BY beer_name\"\n",
    "    with sqlite3.connect(database_path) as conn:\n",
    "        beers = list(pd.read_sql(query,conn)['beer_name'])\n",
    "    beer_options = [{'label': beer, 'value': beer} for beer in beers]\n",
    "    return beer_options\n",
    "\n",
    "\n",
    "\n",
    "# @TODO - replace \n",
    "\n",
    "# # Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "\n",
    "def pipeline_func(data, fns):\n",
    "    return reduce(lambda a, x: x(a), fns, data)\n",
    "\n",
    "\n",
    "#############################################   \n",
    "# 1. Import, Clean, EDA\n",
    "#############################################     \n",
    "\n",
    "# @TODO - rename this function\n",
    "def import_table(db_path, \n",
    "                 query = \"SELECT * FROM user_extract\",\n",
    "                 remove_dups=True):\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    if remove_dups==True:\n",
    "        df = df[~df.duplicated()]\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "def remove_outliers(df, features):\n",
    "    for feature in features:\n",
    "        q1 = df[feature].quantile(.25)\n",
    "        q3 = df[feature].quantile(.75)\n",
    "        iqr = q3 - q1\n",
    "        non_outlier_mask = (df[feature] >= q1 - 1.5*iqr) & (df[feature] <= q3 + 1.5*iqr)\n",
    "        df = df[non_outlier_mask]\n",
    "    return df  \n",
    "\n",
    "def outlier_analysis(df, features, outlier_threshold=2.5):\n",
    "   \n",
    "    print('\\n')\n",
    "    print(\"1. NA Count...\")\n",
    "    print(df.loc[:,features].isna().sum())\n",
    "    \n",
    "    print('\\n')\n",
    "    print('2. Finding IQR outliers...')\n",
    "    features_to_remove = []\n",
    "    for feature in features:\n",
    "        try:\n",
    "            q1 = df[feature].quantile(.25)\n",
    "            q3 = df[feature].quantile(.75)\n",
    "            iqr = q3 - q1\n",
    "            non_outlier_mask = (df[feature] >= q1 - 1.5*iqr) & (df[feature] <= q3 + 1.5*iqr)\n",
    "            outliers = df[~non_outlier_mask]\n",
    "    \n",
    "            print(\"FEATURE {}\".format(feature))\n",
    "            print(\"num of outliers = {:,d}\".format(len(outliers)))\n",
    "            print(\"% of outliers = {:.2f}%\".format(100*len(outliers)/len(df)))\n",
    "\n",
    "            # store feature for outlier removal if necessary\n",
    "            if 100*len(outliers)/len(df) > outlier_threshold:\n",
    "                features_to_remove += [feature]\n",
    "            \n",
    "        except TypeError:\n",
    "            print(\"FEATURE {}\".format(feature))\n",
    "            print(\"ANALYZING ALL NON-NA VALUES\")\n",
    "\n",
    "            non_nas = df[~df[feature].isna()][feature].astype(float)\n",
    "            q1 = non_nas.quantile(.25)\n",
    "            q3 = non_nas.quantile(.75)\n",
    "            iqr = q3 - q1\n",
    "            non_outlier_mask = (non_nas >= q1 - 1.5*iqr) & (non_nas <= q3 + 1.5*iqr)\n",
    "            outliers = non_nas[~non_outlier_mask]\n",
    "            print(\"num of outliers = {:,d}\".format(len(outliers)))\n",
    "            print(\"% of outliers = {:.2f}%\".format(100*len(outliers)/len(non_nas)))\n",
    "\n",
    "            # store feature for outlier removal if necessary \n",
    "            if 100*len(outliers)/len(non_nas) > outlier_threshold:\n",
    "                features_to_remove += [feature]\n",
    "\n",
    "    # remove outliers \n",
    "    print(\"Removing outliers from the following features:\", features_to_remove)\n",
    "    df = remove_outliers(df, features_to_remove)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_na(df, features, impute_method = 'mean'):\n",
    "\n",
    "    for feature in features:\n",
    "        if impute_method == 'mean':\n",
    "            non_nas = df[~df[feature].isna()][feature].astype(float)\n",
    "            feature_mean = non_nas.mean()\n",
    "            df[feature] = df[feature].fillna(feature_mean)\n",
    "        elif impute_method == 0:\n",
    "            non_nas = df[~df[feature].isna()][feature].astype(float)\n",
    "            df[feature] = df[feature].fillna(0)\n",
    "    \n",
    "    print(\"NA Count...\")\n",
    "    print(df.loc[:,features].isna().sum())\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "######################################################     \n",
    "### 2. Cosine Similarity / Nearest Neighbors\n",
    "######################################################     \n",
    "def create_ui_matrix(df, fill_method=0):\n",
    "    # Create User-Item Matrix \n",
    "    data = df\n",
    "    values = 'user_rating'\n",
    "    index = 'username'\n",
    "    columns = 'beer_name'\n",
    "    agg_func = 'mean'\n",
    "    \n",
    "    if fill_method == 'item_mean':\n",
    "        ui_matrix = pd.pivot_table(data=data, values=values, index=index, \n",
    "                                   columns=columns, aggfunc=agg_func)\n",
    "        ui_matrix = ui_matrix.fillna(ui_matrix.mean(axis=0), axis=0)\n",
    "    \n",
    "    elif fill_method == 'user_mean':\n",
    "        ui_matrix = pd.pivot_table(data=data, values=values, index=index, \n",
    "                                   columns=columns, aggfunc=agg_func)\n",
    "        ui_matrix.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    \n",
    "    elif fill_method == 0:\n",
    "        ui_matrix = pd.pivot_table(data=data, values=values, index=index, \n",
    "                                   columns=columns, aggfunc=agg_func, fill_value=0)\n",
    "    else:\n",
    "        raise ValueError(\"Please checkout 'fill_method' value\")\n",
    "    \n",
    "    ui_matrix.columns = list(ui_matrix.columns)\n",
    "    \n",
    "    return(ui_matrix)\n",
    "\n",
    "\n",
    "# c. Calculate Cosine Similarity\n",
    "def calculate_cosine_similarity(user_of_reference, ui_matrix):\n",
    "\n",
    "    # Calculate Cosine Similarity \n",
    "    print(\"User of Reference for Cosine Sim = {}\".format(user_of_reference))\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    X = ui_matrix[ui_matrix.index == user_of_reference]\n",
    "    Y = ui_matrix[ui_matrix.index != user_of_reference]\n",
    "    \n",
    "    sim = cosine_similarity(X,Y)[0].tolist()\n",
    "    names = Y.index\n",
    "    \n",
    "    sim_df = pd.DataFrame({'username':names, 'sim_score':sim})\n",
    "    sim_df = sim_df.sort_values(by='sim_score', ascending=False)\n",
    "    \n",
    "    return(sim_df)\n",
    "\n",
    "\n",
    "def calculate_nearest_neighbors(sim_df):\n",
    "    # add neighbor rank to df\n",
    "    neighbor_rank = sim_df.reset_index(drop=True)\n",
    "    neighbor_rank.index.name = 'nearest_neighbor_rank'\n",
    "    neighbor_rank.reset_index(inplace=True)\n",
    "    neighbor_rank['nearest_neighbor_rank'] = neighbor_rank['nearest_neighbor_rank'] + 1\n",
    "    neighbor_rank = neighbor_rank[['nearest_neighbor_rank', 'username']]\n",
    "    return(neighbor_rank)\n",
    "\n",
    "def merge_nearest_neighobr_rank(df, neighbor_rank):    \n",
    "    print(df.shape)\n",
    "    df = pd.merge(neighbor_rank, df, on='username', how='outer')\n",
    "    print(df.shape)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def COSINE_STEP(df, user_of_reference):\n",
    "    ui_matrix = create_ui_matrix(df)\n",
    "    sim_df = calculate_cosine_similarity(user_of_reference, ui_matrix)\n",
    "    neighbor_rank = calculate_nearest_neighbors(sim_df)\n",
    "    df = merge_nearest_neighobr_rank(df, neighbor_rank)\n",
    "    return df\n",
    "    \n",
    "######################################################   \n",
    "### 3. Scale / Standardize Data \n",
    "######################################################   \n",
    "def transform_features_target(df, features, target):\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(df[features])\n",
    "    df[features] = X_scaler.transform(df[features])\n",
    "    \n",
    "    y_scaler = StandardScaler()\n",
    "    y = np.array(df[target]).reshape(-1, 1 )\n",
    "    y_scaler.fit(y)\n",
    "    df[target] = y_scaler.transform(y)\n",
    "    \n",
    "    feature_scaler = X_scaler\n",
    "    target_scaler = y_scaler \n",
    "    \n",
    "    return(df, feature_scaler, target_scaler)\n",
    "\n",
    "\n",
    "######################################################   \n",
    "### Modeling\n",
    "######################################################  \n",
    "## feature selection\n",
    "def cat_encoding(df, encoding_col):\n",
    "\n",
    "    dummies = pd.get_dummies(df[encoding_col], drop_first=True, prefix=encoding_col)\n",
    "    df = pd.merge(df, dummies, left_index=True, right_index=True)\n",
    "\n",
    "    df.drop(encoding_col, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def count_vectorizer(df, vectoring_col):\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vect = CountVectorizer()\n",
    "    X = vect.fit_transform(df[vectoring_col])\n",
    "    count_df = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
    "    df = pd.concat([df.reset_index(drop=True), count_df], axis=1)\n",
    "    \n",
    "    df.drop(vectoring_col, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tfidf_vectorizer(df, vectoring_col):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vect = TfidfVectorizer()\n",
    "    X = vect.fit_transform(df[vectoring_col])\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
    "    df = pd.concat([df.reset_index(drop=True), tfidf_df], axis=1)\n",
    "    \n",
    "    df.drop(vectoring_col, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## models\n",
    "# CBF \n",
    "def cbf(user_df, algorithm, target, impute_na_mean=False, remove_all_outliers=False, rand_state=12):\n",
    "        \n",
    "    features = list(user_df.columns[user_df.columns != target])\n",
    "    print(\"LEN OF FEATURES\", len(features))\n",
    "    print(\"TOP FEATURES \", features[:10])\n",
    "    print(\"TARGET \", target)\n",
    "    \n",
    "    # impute nan with mean\n",
    "    if impute_na_mean == True:\n",
    "        user_df = impute_na(user_df, features, impute_method='mean')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # remove outliers\n",
    "    if remove_all_outliers == True:\n",
    "        user_df = outlier_analysis(user_df, features, outlier_threshold=0.0)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # train test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(user_df[features], user_df[target], test_size=0.2, random_state=rand_state)\n",
    "    \n",
    "    # setup alg and param space for grid search \n",
    "    if algorithm == 'Lasso':\n",
    "        from sklearn.linear_model import Lasso\n",
    "        model = Lasso(fit_intercept=True, normalize=True, random_state=rand_state)\n",
    "        param_space = {'alpha': np.linspace(0.01, 1.0)}\n",
    "    \n",
    "    elif algorithm == 'Ridge':\n",
    "        from sklearn.linear_model import Ridge\n",
    "        model = Ridge(fit_intercept=True, normalize=True, random_state=rand_state)\n",
    "        param_space = {'alpha': np.linspace(0.01, 1.0)}\n",
    "        \n",
    "    elif algorithm == 'ElasticNet':\n",
    "        from sklearn.linear_model import ElasticNet\n",
    "        model = ElasticNet(fit_intercept=True, normalize=True, random_state=rand_state)\n",
    "        param_space = {'alpha': np.linspace(0.01, 1.0),\n",
    "                       'l1_ratio': np.linspace(0.01, 1.0),}\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Please input a correct algorithm\")\n",
    "        \n",
    "    # gridsearch CV \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    gscv = GridSearchCV(model, param_space, cv=5, scoring='neg_mean_absolute_error', iid=True, refit=True)\n",
    "    gscv.fit(X_train, y_train)\n",
    "    \n",
    "    # get best model \n",
    "    best_model = gscv.best_estimator_\n",
    "    preds = best_model.predict(X_test)\n",
    "\n",
    "    # evaluate performance\n",
    "    error_list = preds - y_test\n",
    "    mse = np.mean(np.array(error_list)**2)\n",
    "    mae = np.absolute(error_list).mean()\n",
    "    quarter_error_perc = 100 * np.sum(np.absolute(error_list) < 0.25) / len(error_list)\n",
    "    half_error_perc = 100 * np.sum(np.absolute(error_list) < 0.50) / len(error_list)\n",
    "    print(\"MSE = {:.2f}\".format(mse))\n",
    "    print(\"MAE = {:.2f}\".format(mae))\n",
    "    print(\"Errors within 0.25 = {:.2f} %\".format(quarter_error_perc))\n",
    "    print(\"Errors within 0.50 = {:.2f} %\".format(half_error_perc))\n",
    "\n",
    "    # fit best model over all data \n",
    "    best_model.fit(user_df[features], user_df[target])\n",
    "    best_params = {}\n",
    "    for key in param_space.keys():\n",
    "        best_params[key] = best_model.get_params()[key]\n",
    "    \n",
    "    return best_model, best_params, mae, quarter_error_perc, half_error_perc\n",
    "\n",
    "\n",
    "# semi-coldstart - collaborative filtering\n",
    "def collaborative_filtering(df, user_of_interest):\n",
    "    try:\n",
    "        df.drop('nearest_neighbor_rank', axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    df = COSINE_STEP(df, user_of_interest)\n",
    "    beer_list = list(df[df['username']==user_of_interest]['beer_name'])\n",
    "    estimated_rating_list = []\n",
    "    error_list = []\n",
    "    for beer in beer_list:\n",
    "        try:\n",
    "            estimated_rating = df[ (df.sort_values('nearest_neighbor_rank')['beer_name'] == beer) & (df['username']!=user_of_interest) ]['user_rating'].iloc[0]\n",
    "            estimated_rating_list.append(estimated_rating)\n",
    "\n",
    "            user_rating = df[(df['username']==user_of_interest) & (df['beer_name']==beer)]['user_rating'].iloc[0].astype(float)\n",
    "            error_list.append(estimated_rating-user_rating)\n",
    "        except IndexError:\n",
    "            print(\"SKIPPING:\", beer)\n",
    "    mse = np.mean(np.array(error_list)**2)\n",
    "    mae = np.absolute(error_list).mean()\n",
    "    quarter_error_perc = 100 * np.sum(np.absolute(error_list) < 0.25) / len(error_list)\n",
    "    half_error_perc = 100 * np.sum(np.absolute(error_list) < 0.50) / len(error_list)\n",
    "\n",
    "    print(\"MSE = {:.2f}\".format(mse))\n",
    "    print(\"MAE = {:.2f}\".format(mae))\n",
    "    print(\"Errors within 0.25 = {:.2f} %\".format(quarter_error_perc))\n",
    "    print(\"Errors within 0.50 = {:.2f} %\".format(half_error_perc))\n",
    "    \n",
    "    return mae, quarter_error_perc, half_error_perc \n",
    "\n",
    "# hybrid\n",
    "def run_hybrid(df, user_of_interest, target):\n",
    "    \n",
    "    features = list(df.columns[df.columns != target])\n",
    "    try:\n",
    "        features.remove('username')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        features.remove('beer_name')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df.drop('nearest_neighbor_rank', axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"LEN OF FEATURES\", len(features))\n",
    "    print(\"TOP FEATURES \", features[:10])\n",
    "    print(\"TARGET \", target)\n",
    "    df = COSINE_STEP(df, user_of_interest)\n",
    "\n",
    "    min_ppu_list = [0, 50, 100, 250, 500, 750, 1000]\n",
    "    n_users_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, \n",
    "                    55, 60, 65, 70, 75, 80, 85, 100, len(df['username'].unique())-1]\n",
    "\n",
    "    mae_list = []\n",
    "    quarter_abs_error_list = []\n",
    "    half_abs_error_list = []\n",
    "    model_list = []\n",
    "\n",
    "\n",
    "    for min_ppu in min_ppu_list:\n",
    "\n",
    "        user_indices = df.username.value_counts()[df.username.value_counts() > min_ppu].index\n",
    "        sub_df = df[df['username'].isin(user_indices)]\n",
    "        n_users_list = [n_users for n_users in n_users_list if n_users < len(user_indices)]\n",
    "\n",
    "        for top_n in n_users_list:\n",
    "\n",
    "            # split data \n",
    "            top_n_nn = list(sub_df['nearest_neighbor_rank'].unique())[:top_n]\n",
    "            df_top_n = df[df['nearest_neighbor_rank'].isin(top_n_nn)]\n",
    "            X_train = df_top_n[features]\n",
    "            y_train = df_top_n[target]\n",
    "            y_train = np.array(y_train).reshape(len(y_train), )\n",
    "\n",
    "            X_test = df[df['username'] == user_of_interest][features]\n",
    "            y_test = df[df['username'] == user_of_interest][target]\n",
    "            y_test = np.array(y_test).reshape(len(y_test), )\n",
    "\n",
    "            # train\n",
    "            from sklearn.linear_model import LassoCV\n",
    "            model = LassoCV(fit_intercept=True, normalize=True, cv=5, random_state=12)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate model on user's data \n",
    "            preds = model.predict(X_test)\n",
    "\n",
    "            # evaluate results\n",
    "            results_df = pd.DataFrame([preds, y_test]).transpose()\n",
    "            results_df.columns = ['predicted', 'actual']\n",
    "            results_df['error'] = results_df['predicted'] - results_df['actual']\n",
    "            results_df['abs_error'] = abs(results_df['error'])\n",
    "\n",
    "            # Performance Metrics \n",
    "            mae = np.mean(results_df['abs_error'])\n",
    "\n",
    "            quarter_abs_error_list.append(100*len(results_df[results_df['abs_error']<=0.25])/len(results_df))\n",
    "            half_abs_error_list.append(100*len(results_df[results_df['abs_error']<=0.50])/len(results_df))\n",
    "            mae_list.append(mae)\n",
    "            model_list.append(model)\n",
    "\n",
    "        # add breaks\n",
    "        quarter_abs_error_list.append(0)\n",
    "        half_abs_error_list.append(0)\n",
    "        mae_list.append(0)\n",
    "        model_list.append(0)\n",
    "        \n",
    "    return model_list, mae_list, quarter_abs_error_list, half_abs_error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_of_interest = 'tsharp93'\n",
    "feature_selection = 'simple'\n",
    "alg = 'Ridge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge\n",
      "LEN OF FEATURES 3\n",
      "TOP FEATURES  ['ABV', 'IBU', 'global_rating']\n",
      "TARGET  user_rating\n",
      "NA Count...\n",
      "ABV              0\n",
      "IBU              0\n",
      "global_rating    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "1. NA Count...\n",
      "ABV              0\n",
      "IBU              0\n",
      "global_rating    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "2. Finding IQR outliers...\n",
      "FEATURE ABV\n",
      "num of outliers = 1\n",
      "% of outliers = 2.50%\n",
      "FEATURE IBU\n",
      "num of outliers = 0\n",
      "% of outliers = 0.00%\n",
      "FEATURE global_rating\n",
      "num of outliers = 1\n",
      "% of outliers = 2.50%\n",
      "Removing outliers from the following features: ['ABV', 'global_rating']\n",
      "MSE = 0.28\n",
      "MAE = 0.45\n",
      "Errors within 0.25 = 12.50 %\n",
      "Errors within 0.50 = 75.00 %\n",
      "{'alpha': 0.01}\n",
      "[0.06483    0.00445568 0.06321167]\n"
     ]
    }
   ],
   "source": [
    "print(alg)\n",
    "# feature prep\n",
    "if feature_selection == 'simple':\n",
    "    df = import_table(db_path, query = \"SELECT username, beer_description, ABV, IBU, global_rating, user_rating FROM prepped_data WHERE username = '{}'\".format(user_of_interest))\n",
    "    user_df = df[df['username']==user_of_interest].drop(['username', 'beer_description'], axis=1, inplace=False)\n",
    "elif feature_selection == 'cat-encoding':\n",
    "    df = import_table(db_path, query = \"SELECT username, beer_description, ABV, IBU, global_rating, user_rating FROM prepped_data\")\n",
    "    df = cat_encoding(df, 'beer_description')\n",
    "    user_df = df[df['username'] == user_of_interest]\n",
    "    user_df.drop(['username'], axis=1, inplace=True)\n",
    "elif feature_selection == 'count-vect':\n",
    "    df = import_table(db_path, query = \"SELECT username, beer_description, ABV, IBU, global_rating, user_rating FROM prepped_data\")\n",
    "    df = count_vectorizer(df, 'beer_description')\n",
    "    user_df = df[df['username'] == user_of_interest]\n",
    "    user_df.drop(['username'], axis=1, inplace=True)\n",
    "elif feature_selection == 'tfidf-vect':\n",
    "    df = import_table(db_path, query = \"SELECT username, beer_description, ABV, IBU, global_rating, user_rating FROM prepped_data\")\n",
    "    df = tfidf_vectorizer(df, 'beer_description')\n",
    "    user_df = df[df['username'] == user_of_interest]\n",
    "    user_df.drop(['username'], axis=1, inplace=True)\n",
    "\n",
    "model, best_params, mae, quarter, half = cbf(user_df, alg, 'user_rating', impute_na_mean=True, remove_all_outliers=True)\n",
    "\n",
    "d={}\n",
    "d['model'] = model\n",
    "d['feature_selection'] = feature_selection\n",
    "\n",
    "print(best_params)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# - remove outliers\n",
    "# - add in scaling_method\n",
    "# - replace features with 'X' in train_test_split\n",
    "# - replace 0 with 'X_scaler in returns of CBF func'\n",
    "# - replace 'fit_intercept' and 'normalize' in Lasso and ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concerns\n",
    "# - outliers\n",
    "# - scaling method\n",
    "# - imputing for prepped_data instead of within each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
